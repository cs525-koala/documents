\section{Implementation}
\label{sec:impl}

Koala builds upon Eucalyptus in four main ways.  First, Koala extends
Eucalyptus by adding live migration, which serves as a fundamental building
block for the other components.  Second, Koala contains performance monitoring
infrastructure to Koala that reports on the nodes and the VMs. Third,
Koala adds a dynamic parameterized scheduler to the cluster controller that
leverages live migration and the monitoring information to achieve more
desirably configurations on-the-fly.  Finally, to overcome the limitations of a
hardcoded scheduler, Koala uses a dynamic learning algorithm to tune the
parameters of the scheduler over time to improve the scheduling decisions to
suit the workload.

\KComment{Describe allllll our important implementation details!!}
\KComment{Do we want to talk about 'roadblocks' here? (Too story-ish?)}

\subsection{Live Migration}
Live migration is the process of transparently moving a running VM between two
machines.  In Koala, in order to efficiently manage the resources of a cloud,
we need to be able to move VMs around to put the system in a more desired
configuration. In particular we need to do so without interrupting service,
including not just execution state but also active network connections.

In the implementation of Koala, we hit an unexpected roadblock.  Eucalyptus did
not support live migration, although the virtualization library it uses,
libvirt, does.  Therefore a large portion of Koala's implementation is the
extensive modifications we made to Eucalyptus (in its state tracking
communication models, etc) to provide for a robust migration implementation we
could build our scheduler on top of.

Libvirt, the Eucalyptus node controller, and the Eucalyptus cluster controllers
all have very different views of the state of VMs.  Libvirt supports specific
states of the VMs in respect to their status on an individual machine. This
means that migration is transparent, and during migration various states
(running, paused, etc) are visible on both machines taking part in migration.
The default node controller and cluster controller are (by design) unaware of
migrating VMs, and quickly report failures when VMs are migrated unexpectedly
(this can be done easily by issuing libvirt commands outside of eucalyptus) as
their state becomes confused.  The node controllers had to be modified to
account for the respective states (both in respect to libvirt and migration),
and to be robust to a wide variety of cases such as internal shutdown or VM/node
failure.  The cluster controller has to manage its respective state for both
nodes in addition to transmitting the relevant Node Controller state for the VMs
and the controller itself.  Additionally the cluster controller has to reason
about this migration state when reporting to the cloud the status of a
particular VM.

The very nature of migration involves slightly breaking the Eucalyptus model of
communication between controllers.  Initially, nodes only communicated with
their respective cluster controller and (ignoring what occurs in the VMs
themselves) are completely oblivious to each others existence.  The process of
migration fundamentally involves sending the entirety of the state of a VM (in
the form of a disk image, kernel, ramdisk, and memory pages) to another node.
Short of redirecting this communication through the cluster, which would
generate significant bandwidth, or using special routing hardware to mask the
transmission, migration would not be possible without exposing the existence and
location of other nodes.

Both nodes taking part in migration need to have access to the VM image.  We set
up a network file-system over NFS, where we store the VM images.  Accessing a VM
image off the network is a very bandwidth expensive operation, so we utilized
the Eucalyptus image caching functionality, in order to keep bandwidth costs of
both operation and migration to a minimum.

Putting the disks backing the instances on shared storage is an important design
decision.  Depending on the locking and caching of the filesystem, doing this
might greatly hinder system performance for I/O intensive instances.  We hope to
explore this trade-off more in the future, and perhaps look into ideas such as
sending the disk contents on-demand, or making use of copy-on-write snapshots to
increase performance for these workloads.

\subsection{Scheduling}


Our scheduler's implementation is primarily as described in
Algorithm~\ref{algo:sched}, with some exceptions.
\KComment{List these exceptions, or remove this subsection entirely}

\subsection{Monitoring Infrastructure}

\KComment{WRITE ME}

\subsection{Scoring Algorithm}

Our scoring algorithm currently only utilizes CPU related information to score the configurations of the system.  As additional performance monitoring infrastructure is added, the respective components of the scoring algorithm will be added as well.

\subsection{Data Mining + Learning Algorithms}

Our implementation presently does not make use of any learning algorithms,
primarily due to lack of sufficient training time.  An extensive evaluation is required
to demonstrate the effectiveness of such algorithms, and to build a sufficiently
large data set to train the learning on.  In the future, we will use established
machine learning techniques to generate inputs for the thresholds in our
scheduler in order to adapt to the properties a particular workload and cluster.
We also expect to find the trade-off values we gather from such mining to be
useful contributions to the literature.

%\subsection{Summary}
%
%Although both live migration and more advanced scheduling improve resource
%consumption on the cluster, these techniques are at a cost.  Live migration
%causes the nodes involved to incur the cost of managing the migration, as well
%as imposes a burden on the network while transferring across the related VM
%state.  The scheduling itself, particularly with the introduction of learning
%algorithms adds overhead to the cluster controller.  Finally, migration itself
%might incur some degraded performance in the guest machine.
%
%The actual computation cost of migration is very small in comparison to an
%instance running for an extended amount of time.  Detail of this can be found in
%our evaluation.  In addition to the performance gains, we gain the ability to
%switch the system to be optimized for energy efficiency on the fly.  The larger
%costs associated with Koala would be the analysis of the event data and the
%generation of decision trees and other comparable structures.  As our
%implementation permits, we plan to evaluate the trade-off between these
%computations and the benefits gained from more optimal scheduling.  It's
%important to note that many of these costs (what does it 'cost' to migrate a
%machine with respect to some optimality metric?) will be handled by Koala's
%learning dynamic scheduler, which we believe will help mitigate many of
%these costs.

