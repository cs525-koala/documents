\section{Design}
\label{sec:design}

%%\KComment{In general, many parts of the design are not motivated: they are just
%%presented as is. This makes it difficult to convince the reader that the
%%design decisions were in fact the best ones possible.}
%%
%%\KComment{Sec 3 presents several different components: it is not clear however
%%why you absolutely need these different components. Give examples here to make
%%the motivation clearer.}

Koala's primary addition to Eucalyptus is its dynamic scheduler.  The scheduler
attempts to improve resource utilization by reallocating resources on-the-fly.
To support this, Koala consists of two additional components: live migration
support, and performance monitoring infrastructure.  Finally, to be general
enough to be suitable for many deployments, but to still be aggressive for a
particular environment, we add a learning algorithm.  This allows us to follow
longer trends but still be responsive to small changes.

Figure~\ref{fig:Koala} shows these components and where they fit into the
overall system architecture, and we describe each of them below.

\scalefig{Koala}{0.9}{Koala System Overview}

\subsection{Dynamic Scheduler}
%\KComment{Sec 3.2 is rather vague and abstract: please tighten.
%information..configuration parameters..some optimality metric.¹ What
%monitoring information is used? Which configuration params are set (and why
%only those)? What optimality metrics? Give examples.}

The most important part of Koala is its dynamic scheduler.  The addition of this
scheduler enables Koala to make better use of the available resources, and to
respond to changing loads and system conditions.

Koala's dynamic scheduler's primary output is a single migration that it
believes will work towards improving the system.  In order to determine what a
better configuration is, we define a score $Score(M,C)$ for monitoring
information $M$ and a particular configuration $C$.  A higher score means a
preferred configuration, one that makes better use of resources.  The scheduler
uses this scoring metric to find a better configuration and work to get there.

When considering the maximum throughput of the system, we make some assumptions
about the system.  One of these is that the system will perform best when each
VM is given as many resources it needs, and that we can roughly estimate
system performance as the integral of our scoring metric over time.  That is to
say that getting a VM a needed resource earlier in its execution will
make the system better, and that trades-off against the amount of resource
needed and when it has it.  We believe this to be reasonable assumption to make
about real-world tasks.
\KComment{Word this better, and can we really claim 'reasonable'?}
\KComment{I believe there's a name for this, continuous ..something?}

\input{scheduler.tex}

The scoring metric we use is listed as Algorithm~\ref{algo:score} and the scheduler is
listed as Algorithm~\ref{algo:sched}.  The scheduler runs every $T$ seconds (in
practice we set $T$ to 30), and upon execution it looks through $D$ steps of
(legal) migrations, and computes the scores of the resulting configurations.
Taking into account a heuristic cost of migration $migrationCost$, it determines
what the best next step is.  Each additional migration used to reach a desired
configuration incurs an additional penalty.   This penalty can be used to
prevent the algorithm from looking too far ahead and never making sufficient
local progress.  At the moment since $D$ is small (we used $2$), we do not worry
about the cost of intermediate configurations when determining what steps to
take.  Setting parameters like $T$ and $D$ are something we defer to the
learning scheduler: setting $D$ too small and we might too aggressively seek
local optimums, too large and we might make constantly short-term bad decisions
and never reach the ideal (perhaps the VM's jobs are done by then).  These
complexities and those surrounding other parameters in our system motivate the
learning scheduler to change them in response to observed longer-term
performance feedback.

The scoring metric is relatively simple, and largely relies on a robust and
powerful performance monitoring infrastructure.  Algorithm~\ref{algo:score}
shows how we compute the score for a particular configuration using monitoring
information $M$.  We monitor VMs and look to determine their theoretical
resource consmuption use, for example a VM running on a CPU-overloaded
node (perhaps it has many non-VM background tasks) might internaly be wanting to
use 100\% of it's CPU, but we observe that its virtual cpu only is being
scheduled 50\%.  This difference is what matters, and what we can use to
determine when a VM is better off elsewhere.  Finally, we declare
that if the VM's on a node are not using all of a particular resource, that
should be scored the same as if the node has exactly the 'right' amount of that
resource.  This is to prevent unnecessary migration to more capable hosts
because unless more is needed, the VMs in question will make 'maximal'
progress using the node they're on (this is reflected in
line~\ref{algo:scoremax}).

%\subsection{Live Migration}
%Live migration is the process of transparently moving a running VM between two
%machines.  In Koala, in order to efficiently manage the resources of a cloud,
%we need to be able to move VMs around to put them in a desired configuration.
%This must be done without disrupting the internal state of the VM, any active
%network connections, and that while moving VMs we have minimal downtime.  Koala
%uses live migration to achieve this.  This live migration primitive is a key
%part of the Koala architecture, enabling the other components to dynamically
%manage the resources of the cluster.
%
%\subsection{Performance Monitoring}
%\KComment{WRITE ME}
%
%
%\subsection{Dynamic Learning}
%\KComment{Sec 3.3 is abstract and says that Œit uses monitoring}
%Given our parameterized scheduler, we have the task of setting the
%configuration values to something reasonable for a particular system and set of
%workloads, all of which might change dynamically.  We believe asking the user
%to set these values imposes an unreasonable burden, due to both the complexity
%of such a task and the ever-changing needs of an Eucalyptus system.  Koala
%simplifies this task by leveraging learning algorithms.  It uses monitoring
%information to attempt to find ideal values for the configuration parameters,
%with respect to some optimality metric.  The problem of how to determine the
%optimality metric is in itself, a complicated issue.  This is due to the fact
%that there are many different definitions of optimal (attempts to reduce power
%consumption might be at odds with attempts to increase system performance), and
%so we generalize these to a smaller set of knobs which are exposed to the
%end-user.  This greatly simplifies the burden on the user, and because this too
%is dynamically configurable, can allow for greater control of the system as
%requirements and priorities change.
%
%\subsection{Summary}
%
%Koala has three components, all of which build upon each other.  The addition
%of live migration allows dynamic resource management, but is far too
%complicated for Eucalyptus's primitive scheduler to reason about.  The
%parameterized scheduler attempts to uses live migration to achieve a more
%desirable system configuration, but exposes a new challenge in the form of
%configurable parameters.  Learning algorithms, when provided sufficient data
%and optimality metrics, should provide near optimal scheduling parameters.  We
%believe this is a powerful combination that not only greatly simplifies and
%automates the resource management of the cluster, but also enables
%significantly improved resource utilization than would otherwise be possible.
