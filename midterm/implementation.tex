\section{Implementation}

\subsection{Live Migration}

%% WD: I feel like this is way too 'story'-y.  Is this a valid concern?

In the implementation of our project, we hit an unexpected roadblock.  Eucalyptus did not support live migration, a method for moving running VMs between nodes.  Unfortunately we had intended to utilize live migration heavily within our scheduler, so we set forth to implement it.  We modified Eucalyptus to support live migration.  This involved leveraging libvirt, the virtualization toolkit used by Eucalyptus, in order to perform the migration.  Additionally, extensive modification of Eucalyptus was required to communicate with libvirt and to manage its state during migration.  

% Do we sufficiently explain these components in the background section?
Libvirt, the Eucalyptus node controller, and the Eucalyptus cluster controllers all have very different views of the state of VMs.  Libvirt supports specific states of the VMs in respect to their status on an individual machine. This means that migration is transparent, and during migration various states (running, paused, etc) are visible on both machines taking part in migration.  The default node controller and cluster controller are (by design) unaware of migrating VMs, and quickly report failures when VMs are migrated unexpectedly (this can be done easily by issuing libvirt commands outside of eucalyptus) as their state becomes confused.  The node controllers had to be modified to account for the respective states (both in respect to libvirt and migration), and to be robust to a wide variety of cases such as internal shutdown or VM/node failure.  The cluster controller has to manage its respective state for both nodes in addition to transmitting the relevant Node Controller state for the VMs and the controller itself.  Additionally the cluster controller has to reason about this migration state when reporting to the cloud the status of a particular instance.

The very nature of migration involves slightly breaking the Eucalyptus model of communication between controllers.  Initially, nodes only communicated with their respective cluster controller and (ignoring what occurs in the instances themselves) are completely oblivious to each others existence.  The process of migration fundamentally involves sending the entirety of the state of a VM (in the form of a disk image, kernel, ramdisk, and memory pages) to another node.  Short of redirecting this communication through the cluster, which would generate significant bandwidth, or using special routing hardware to mask the transmission, migration would not be possible without exposing the existence and location of other nodes.  

% WD: Discuss shared storage architecture as solution to not sending the disk image and whatnot node<-->node?


\subsection{Scheduling}

Currently, our scheduler is quite elementary and only exposes two thresholds, one to adjust the scheduling frequency, and one in which to optimize between throughput and energy efficiency.  Our model of throughput is currently built on the assumption that instances assigned to individual machines will perform better than the same number of instances on fewer machines, and our energy model is based on the assumption that instances assigned to individual machines on consume more energy than the same number of instances on fewer machines.  We will implement a much greater number of thresholds to implement.  One of these thresholds will adjust the VM densities and the hardware resources allocated to the specific VMs.  Another threshold would prioritize data locality over general computational throughput.  These knobs will allow us to modify the ways in which VMs are allocated, in terms of both allocations and migrations, in order to better meet the needs of the cluster.

\subsection{Data Mining + Learning Algorithms}

A lot of work has been done in the areas of data mining and machine learning. ******<INSERT OBLIGATORY REFERENCES HERE>*******  We intend to leverage this work by exposing various scheduling and machine events.  We will use established machine learning techniques to generate inputs for the thresholds in our scheduler in order to adapt to the properties of our workload and cluster.  A more optimized scheduler can better allocate the VMs to the resources of the cluster, and should achieve higher throughput.

\subsection{Summary}

Although both live migration and more advanced scheduling improve resource consumption on the cluster, these techniques are at a cost.  Live migration causes the nodes involved to incur the cost of managing the migration, as well as imposes a burden on the network while transferring across the related VM state.  The scheduling itself, particularly with the introduction of learning algorithms adds overhead to the cluster controller.  Finally, migration itself might incur some degraded performance in the guest machine.

The actual computation cost of migration is very small in comparison to an instance running for an extended amount of time.  Detail of this can be found in our evaluation.  In addition to the performance gains, we gain the ability to switch the system to be optimized for energy efficiency on the fly.  The larger costs associated with Koala would be the analysis of the event data and the generation of decision trees and other comparable structures.  As our implementation permits, we plan to evaluate the trade-off between these computations and the benefits gained from more optimal schedulings.  It's important to note that many of these costs (what does it 'cost' to migrate a machine with respect to some optimality metric?) will be handled by Koala's learning dynamic scheduler, which we believe will help mitigate many of these costs.
