\section{Implementation}

In the implementation of our project, we hit an unexpected roadblock.  Eucalyptus did not support live migration, a method for moving running VMs between nodes.  Unfortunately we had intended to utilize live migration heavily within our scheduler, so we set forth to implement it.  We modified Eucalyptus to support live migration.  This involved leveraging libvirt, a virtualizaton api, in order to perform the migration.  Additionally, extensive modification of Eucalyptus was required to communicate with libvert and to manage its state during migration.  

Libvert, the Eucalyptus node controller, and the Eucalyptus cluster controllers all have very different views of the state of VMs.  Libvirt supports specific states of the VMs in respect to their status on an individual machine. This means that migration is transparent, and during migration, various states (running, paused, etc), are visible on both machines taking part in migration.  The default node controller and cluster controller are (by design) unaware of migrating VMs, and quickly report failures when VMs are migrated unexpectedly (this can be done easily by issuing libvirt commands outside of eucalyptus) as their state becomes confused.  The node controllers had to be modified to account for the respective states (both in respect to libvirt and migration), and to be robust to a wide variety of cases such as internal shutdown or VM/node failure.  The cluster controller has to manage its respective state for both nodes in addition to transmitting the relevant Node Controller state for the VMs and the controller itself.  

The very nature of migration involves slightly breaking the Eucalyptus model of communication between controllers.  Initially, nodes only communicated with their respective cluster controller and (ignoring what occurs in the instances themselves), are completely oblivious to each others existence.  The process of migration fundamentally involves sending the entirety of the state of a VM(in the form of a disk image, kernel, ramdisk, and memory pages) to another node.  Short of redirecting this communication through the cluster, which would generate significant bandwidth, or using special routing hardware to mask the transmission, migration would not be possible without exposing the existence and location of other nodes.


