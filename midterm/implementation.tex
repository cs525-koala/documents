\section{Implementation}

In the implementation of our project, we hit an unexpected roadblock.  Eucalyptus did not support live migration, a method for moving running VMs between nodes.  Unfortunately we had intended to utilize live migration heavily within our scheduler, so we set forth to implement it.  We modified Eucalyptus to support live migration.  This involved leveraging libvirt, a virtualizaton api, in order to perform the migration.  Additionally, extensive modification of Eucalyptus was required to communicate with libvirt and to manage its state during migration.  

Libvirt, the Eucalyptus node controller, and the Eucalyptus cluster controllers all have very different views of the state of VMs.  Libvirt supports specific states of the VMs in respect to their status on an individual machine. This means that migration is transparent, and during migration, various states (running, paused, etc), are visible on both machines taking part in migration.  The default node controller and cluster controller are (by design) unaware of migrating VMs, and quickly report failures when VMs are migrated unexpectedly (this can be done easily by issuing libvirt commands outside of eucalyptus) as their state becomes confused.  The node controllers had to be modified to account for the respective states (both in respect to libvirt and migration), and to be robust to a wide variety of cases such as internal shutdown or VM/node failure.  The cluster controller has to manage its respective state for both nodes in addition to transmitting the relevant Node Controller state for the VMs and the controller itself.  

The very nature of migration involves slightly breaking the Eucalyptus model of communication between controllers.  Initially, nodes only communicated with their respective cluster controller and (ignoring what occurs in the instances themselves), are completely oblivious to each others existence.  The process of migration fundamentally involves sending the entirety of the state of a VM(in the form of a disk image, kernel, ramdisk, and memory pages) to another node.  Short of redirecting this communication through the cluster, which would generate significant bandwidth, or using special routing hardware to mask the transmission, migration would not be possible without exposing the existence and location of other nodes.

Currently, our scheduler is quite elementary and only exposes two thresholds, one to adjust the scheduling frequency, and one in which to optimize between throughput and energy efficiency.  Our model of throughput is currently built on the assumption that instances assigned to individual machines will perform better than the same number of instances on fewer machines, and our energy model is based on the assumption that instances assigned to individual machines on consume more energy than the same number of instances on fewer machines.  We will implement a much greater number of thresholds to implement such as <INSERT MORE KNOBS HERE>.  These knobs will allow us to modify the ways in which VMs are allocated, in terms of both allocations and migrations, in order to better meet the needs of the cluster.

Although both live migration and more advanced scheduling will improve performance, both of these techniques will impose some amount of computational overhead on the system.  Over the duration of live migration, both the sending and receiving nodes will experience computational overhead.  Additionally, network bandwidth will be used in order to send memory pages between these machines.  The cluster controller will also have a small overhead from the scheduling process.  

The actual computation cost of migration is very small in comparison to an instance running for an extended amount of time.  Detail of this can be found in our evaluation.  In addition to the performance gains, we gain the ability to switch the system to be optimized for energy efficiency on the fly.